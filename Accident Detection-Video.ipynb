{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93c20982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ramnivash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2     # for capturing videos\n",
    "import math \n",
    "import geocoder\n",
    "import requests\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from twilio.rest import Client\n",
    "from geopy.geocoders import Nominatim\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "from keras import utils\n",
    "from matplotlib import pyplot as plt \n",
    "from skimage.transform import resize   # for resizing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d50c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "videoFile = \"C:/Users/Ramnivash/Documents/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main (1)/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main/Accidents.mp4\"\n",
    "cap = cv2.VideoCapture(videoFile)   # capturing the video from the given path\n",
    "frameRate = cap.get(5) #frame rate\n",
    "x=1\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1ec509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b3a7d87c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = plt.imread('0.jpg')   # reading image using its name\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da41986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_ID</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image_ID  Class\n",
       "0    0.jpg      1\n",
       "1    1.jpg      1\n",
       "2    2.jpg      1\n",
       "3    3.jpg      1\n",
       "4    4.jpg      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/Ramnivash/Documents/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main (1)/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main/mapping.csv\")     # reading the csv file\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39a7f413",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ ]     # creating an empty array\n",
    "for img_name in data.Image_ID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    X.append(img)  # storing each image in array X\n",
    "X = np.array(X)    # converting list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8119c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.Class\n",
    "dummy_y = utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f7cf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "for i in range(0,X.shape[0]):\n",
    "    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3\n",
    "    image.append(a)\n",
    "X = np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2555ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "X = preprocess_input(X,data_format=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21006e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af20a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d94402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ramnivash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Ramnivash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb02ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 46s 9s/step\n",
      "3/3 [==============================] - 20s 5s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((155, 7, 7, 512), (67, 7, 7, 512))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = base_model.predict(X_train)\n",
    "X_valid = base_model.predict(X_valid)\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "555111d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(155, 7*7*512)      # converting to 1-D\n",
    "X_valid = X_valid.reshape(67, 7*7*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5a6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train/X_train.max()      # centering the data\n",
    "X_valid = X_valid/X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57001716",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer((7*7*512,)))    # input layer\n",
    "model.add(Dense(units=1024, activation='sigmoid')) # hidden layer\n",
    "model.add(Dense(2, activation='softmax'))    # output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "586cff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              25691136  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25693186 (98.01 MB)\n",
      "Trainable params: 25693186 (98.01 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e36420f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ramnivash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20e00a1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\Ramnivash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Ramnivash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5/5 [==============================] - 3s 387ms/step - loss: 0.8038 - accuracy: 0.5871 - val_loss: 0.8548 - val_accuracy: 0.5970\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.4949 - accuracy: 0.7677 - val_loss: 0.7685 - val_accuracy: 0.7313\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.2760 - accuracy: 0.8903 - val_loss: 0.7723 - val_accuracy: 0.7463\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.1347 - accuracy: 0.9613 - val_loss: 0.8551 - val_accuracy: 0.7015\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.1100 - accuracy: 0.9742 - val_loss: 0.8042 - val_accuracy: 0.7612\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.0767 - accuracy: 1.0000 - val_loss: 0.8343 - val_accuracy: 0.7463\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0517 - accuracy: 0.9935 - val_loss: 0.8446 - val_accuracy: 0.7612\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.8609 - val_accuracy: 0.7463\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 0.8526 - val_accuracy: 0.7612\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 0.8376 - val_accuracy: 0.7612\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 0.8331 - val_accuracy: 0.7612\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.8352 - val_accuracy: 0.8060\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.8457 - val_accuracy: 0.7761\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.8506 - val_accuracy: 0.7761\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.8584 - val_accuracy: 0.7463\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.8615 - val_accuracy: 0.7463\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.8638 - val_accuracy: 0.7761\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.8630 - val_accuracy: 0.7761\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.8622 - val_accuracy: 0.7761\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 250ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.8616 - val_accuracy: 0.7761\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.8643 - val_accuracy: 0.7761\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.8645 - val_accuracy: 0.7761\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.8656 - val_accuracy: 0.7910\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.8708 - val_accuracy: 0.7761\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 0.8731 - val_accuracy: 0.7761\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.8772 - val_accuracy: 0.7761\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.8786 - val_accuracy: 0.7761\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 231ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.8773 - val_accuracy: 0.7910\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 233ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.8793 - val_accuracy: 0.7910\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 256ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.8838 - val_accuracy: 0.7910\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.8860 - val_accuracy: 0.7910\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.8894 - val_accuracy: 0.7910\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8913 - val_accuracy: 0.7761\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.8921 - val_accuracy: 0.7910\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.8950 - val_accuracy: 0.7910\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.8962 - val_accuracy: 0.7910\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.8965 - val_accuracy: 0.7910\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.8979 - val_accuracy: 0.7910\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 225ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.8997 - val_accuracy: 0.7910\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.9023 - val_accuracy: 0.7910\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.9059 - val_accuracy: 0.7910\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.9077 - val_accuracy: 0.7910\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 228ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.9074 - val_accuracy: 0.7910\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.9090 - val_accuracy: 0.8060\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.9097 - val_accuracy: 0.8060\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.9130 - val_accuracy: 0.8060\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 248ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.9145 - val_accuracy: 0.8060\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.9161 - val_accuracy: 0.8060\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.9180 - val_accuracy: 0.8060\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 223ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.9200 - val_accuracy: 0.8209\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.9227 - val_accuracy: 0.8209\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.9235 - val_accuracy: 0.8209\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.9247 - val_accuracy: 0.8209\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9265 - val_accuracy: 0.8209\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.9270 - val_accuracy: 0.8209\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 254ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9294 - val_accuracy: 0.8209\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.9308 - val_accuracy: 0.8209\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 227ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9326 - val_accuracy: 0.8209\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.9342 - val_accuracy: 0.8209\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9345 - val_accuracy: 0.8209\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 224ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.9360 - val_accuracy: 0.8209\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 248ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.8209\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 241ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9374 - val_accuracy: 0.8209\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 2s 337ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9392 - val_accuracy: 0.8209\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 2s 319ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9414 - val_accuracy: 0.8209\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 2s 372ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.9435 - val_accuracy: 0.8209\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 2s 335ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.9454 - val_accuracy: 0.8209\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.9468 - val_accuracy: 0.8209\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9479 - val_accuracy: 0.8209\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9490 - val_accuracy: 0.8209\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9506 - val_accuracy: 0.8209\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9512 - val_accuracy: 0.8209\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 243ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9518 - val_accuracy: 0.8209\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9529 - val_accuracy: 0.8209\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9550 - val_accuracy: 0.8209\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9575 - val_accuracy: 0.8209\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9592 - val_accuracy: 0.8209\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9600 - val_accuracy: 0.8209\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9614 - val_accuracy: 0.8209\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.8209\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9642 - val_accuracy: 0.8209\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 270ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9652 - val_accuracy: 0.8209\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 237ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9657 - val_accuracy: 0.8209\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9662 - val_accuracy: 0.8209\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 240ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9673 - val_accuracy: 0.8209\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 9.8274e-04 - accuracy: 1.0000 - val_loss: 0.9690 - val_accuracy: 0.8209\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 9.6478e-04 - accuracy: 1.0000 - val_loss: 0.9703 - val_accuracy: 0.8209\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 234ms/step - loss: 9.4592e-04 - accuracy: 1.0000 - val_loss: 0.9713 - val_accuracy: 0.8209\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 316ms/step - loss: 9.2915e-04 - accuracy: 1.0000 - val_loss: 0.9727 - val_accuracy: 0.8209\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 235ms/step - loss: 9.0971e-04 - accuracy: 1.0000 - val_loss: 0.9740 - val_accuracy: 0.8209\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 242ms/step - loss: 8.9392e-04 - accuracy: 1.0000 - val_loss: 0.9748 - val_accuracy: 0.8209\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 8.7613e-04 - accuracy: 1.0000 - val_loss: 0.9755 - val_accuracy: 0.8209\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 8.6137e-04 - accuracy: 1.0000 - val_loss: 0.9767 - val_accuracy: 0.8209\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 244ms/step - loss: 8.4502e-04 - accuracy: 1.0000 - val_loss: 0.9783 - val_accuracy: 0.8209\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 236ms/step - loss: 8.2835e-04 - accuracy: 1.0000 - val_loss: 0.9790 - val_accuracy: 0.8209\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 8.1303e-04 - accuracy: 1.0000 - val_loss: 0.9802 - val_accuracy: 0.8209\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 268ms/step - loss: 7.9940e-04 - accuracy: 1.0000 - val_loss: 0.9810 - val_accuracy: 0.8209\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 247ms/step - loss: 7.8641e-04 - accuracy: 1.0000 - val_loss: 0.9823 - val_accuracy: 0.8209\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 239ms/step - loss: 7.7229e-04 - accuracy: 1.0000 - val_loss: 0.9824 - val_accuracy: 0.8209\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 238ms/step - loss: 7.5879e-04 - accuracy: 1.0000 - val_loss: 0.9838 - val_accuracy: 0.8209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b38d1e00d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, y_train, epochs=100, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "494768fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8d52fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "videoFile = \"C:/Users/Ramnivash/Documents/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main (1)/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main/Accident-1.mp4\"\n",
    "cap = cv2.VideoCapture(videoFile)\n",
    "frameRate = cap.get(5) #frame rate\n",
    "x=1\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"test%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf0f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/Ramnivash/Documents/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main (1)/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9047ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = []\n",
    "for img_name in test.Image_ID:\n",
    "    img = plt.imread('' + img_name)\n",
    "    test_image.append(img)\n",
    "test_img = np.array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f32558f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = []\n",
    "for i in range(0,test_img.shape[0]):\n",
    "    a = resize(test_img[i], preserve_range=True, output_shape=(224,224)).astype(int)\n",
    "    test_image.append(a)\n",
    "test_image = np.array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff1260b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 7, 7, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing the images\n",
    "test_image = preprocess_input(test_image, data_format=None)\n",
    "\n",
    "# extracting features from the images using pretrained model\n",
    "test_image = base_model.predict(test_image)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9f69282",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_image.reshape(9, 7*7*512)\n",
    "\n",
    "# zero centered images\n",
    "test_image = test_image/test_image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "721c02ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 169ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb5b165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0302963e-06 9.9999702e-01]\n",
      " [1.5038604e-04 9.9984956e-01]\n",
      " [1.6737680e-03 9.9832624e-01]\n",
      " [6.1606057e-04 9.9938393e-01]\n",
      " [1.5566851e-03 9.9844331e-01]\n",
      " [3.2684079e-01 6.7315924e-01]\n",
      " [6.7474282e-01 3.2525718e-01]\n",
      " [5.0388473e-01 4.9611527e-01]\n",
      " [7.4214953e-01 2.5785050e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f040ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Accident\n",
      "No Accident\n",
      "No Accident\n",
      "No Accident\n",
      "No Accident\n",
      "No Accident\n",
      "Accident\n",
      "Accident\n",
      "Accident\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,9):\n",
    "    if predictions[i][0]<predictions[i][1]:\n",
    "        print(\"No Accident\")\n",
    "    else:\n",
    "        print(\"Accident\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9df4892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 555ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 370ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 1s 630ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 1s 571ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 1s 681ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 473ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 1s 507ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 1s 511ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 420ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "videoFile = \"C:/Users/Ramnivash/Documents/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main (1)/ACCIDENT-DETECTION-WITH-A-REPORTING-SYSTEM-main/Accident-1.mp4\"\n",
    "cap = cv2.VideoCapture(videoFile)\n",
    "frameRate = cap.get(5) #frame rate\n",
    "x=1\n",
    "predictions_list = []  # List to store predictions\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    if (frameId % math.floor(frameRate) == 0):\n",
    "        filename =\"test%d.jpg\" % count\n",
    "        cv2.imwrite(filename, frame)\n",
    "        \n",
    "        # Read the saved image\n",
    "        img = plt.imread(filename)\n",
    "        img = resize(img, preserve_range=True, output_shape=(224,224)).astype(int)\n",
    "        img = np.expand_dims(img, axis=0)  # Expand dimensions to match model input\n",
    "        \n",
    "        # Preprocess image\n",
    "        img = preprocess_input(img, data_format=None)\n",
    "        \n",
    "        # Extract features using pre-trained model\n",
    "        img_features = base_model.predict(img)\n",
    "        img_features = img_features.reshape(1, 7*7*512)\n",
    "        \n",
    "        # Zero-centering\n",
    "        img_features = img_features / img_features.max()\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model.predict(img_features)\n",
    "        predictions_list.append(prediction)\n",
    "        \n",
    "        # Overlay prediction onto frame\n",
    "        if prediction[0][0] < prediction[0][1]:\n",
    "            cv2.putText(frame, \"No Accident\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        else:\n",
    "            cv2.putText(frame, \"Accident\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Save the frame with prediction\n",
    "        cv2.imwrite(\"prediction_\" + filename, frame)\n",
    "        \n",
    "        count += 1\n",
    "\n",
    "cap.release()\n",
    "print(\"Done!\")\n",
    "\n",
    "# Now you have predictions for each frame stored in predictions_list\n",
    "# You can use these predictions to create a new video with prediction overlays if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dd6eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dbcbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a76e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a6a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aaa0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab12e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38071c34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
